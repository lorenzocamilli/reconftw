#!/bin/bash

# Load the environment variables from the configuration file
# Load environment variables
if [[ -f "reconftw.cfg" ]]; then
    source reconftw.cfg
else
    echo "Error: reconftw.cfg not found!"
    exit 1
fi

# Function to display the help menu
help_menu() {
    echo "Usage: sub_scraping.sh [OPTIONS]"
    echo ""
    echo "Options:"
    echo "    -h, --help            Display this help menu"
    echo "    --deep                Run in deep mode"
    echo "    --no-axiom            Disable Axiom"
    echo "    --diff                Enable DIFF mode"
}

# Input validation and options parsing
while [[ "$#" -gt 0 ]]; do
    case $1 in
        -h|--help) help_menu; exit 0 ;;
        --deep) DEEP=true; shift ;;
        --no-axiom) AXIOM=false; shift ;;
        --diff) DIFF=true; shift ;;
        *) echo "Unknown parameter: $1"; help_menu; exit 1 ;;
    esac
done

# The main sub_scraping function
sub_scraping() {
	touch .tmp/scrap_subs.txt
	if [[ -s ""${dir}"/subdomains/subdomains.txt" ]]; then
		if [[ $(cat subdomains/subdomains.txt | wc -l) -le $DEEP_LIMIT ]] || [[ "$DEEP" = true ]] ; then
			if [[ ! ${AXIOM} = true ]]; then
				resolvers_update_quick_local
				cat subdomains/subdomains.txt | httpx -follow-host-redirects -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info1.txt 2>>"${LOGFILE}" >/dev/null
				[[ -s ".tmp/web_full_info1.txt" ]] && cat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains 2>>"${LOGFILE}" | anew -q .tmp/scrap_subs.txt
				[[ -s ".tmp/probed_tmp_scrap.txt" ]] && cat .tmp/probed_tmp_scrap.txt | httpx -tls-grab -tls-probe -csp-probe -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info2.txt 2>>"${LOGFILE}" >/dev/null
				[[ -s ".tmp/web_full_info2.txt" ]] && cat .tmp/web_full_info2.txt | jq -r 'try ."tls-grab"."dns_names"[],try .csp.domains[],try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | sort -u | httpx -silent | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains 2>>"${LOGFILE}" | anew -q .tmp/scrap_subs.txt
				if [[ "$DEEP" = true ]]; then
					[[ -s ".tmp/probed_tmp_scrap.txt" ]] && katana -silent -list .tmp/probed_tmp_scrap.txt -jc -kf all -c $KATANA_THREADS -d 3 -fs rdn -o .tmp/katana.txt 2>>"${LOGFILE}" >/dev/null
				else
					[[ -s ".tmp/probed_tmp_scrap.txt" ]] && katana -silent -list .tmp/probed_tmp_scrap.txt -jc -kf all -c $KATANA_THREADS -d 2 -fs rdn -o .tmp/katana.txt 2>>"${LOGFILE}" >/dev/null
				fi
			else
				resolvers_update_quick_axiom
				axiom-scan subdomains/subdomains.txt -m httpx -follow-host-redirects -random-agent -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info1.txt "${AXIOM_EXTRA_ARGS}" 2>>"${LOGFILE}" >/dev/null
				[[ -s ".tmp/web_full_info1.txt" ]] && cat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains 2>>"${LOGFILE}" | anew -q .tmp/scrap_subs.txt
				[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m httpx -tls-grab -tls-probe -csp-probe -random-agent -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info2.txt "${AXIOM_EXTRA_ARGS}" 2>>"${LOGFILE}" >/dev/null
				[[ -s ".tmp/web_full_info2.txt" ]] && cat .tmp/web_full_info2.txt | jq -r 'try ."tls-grab"."dns_names"[],try .csp.domains[],try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | sort -u | httpx -silent | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains 2>>"${LOGFILE}" | anew -q .tmp/scrap_subs.txt
				if [[ "$DEEP" = true ]]; then
					[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m katana -jc -kf all -d 3 -fs rdn -o .tmp/katana.txt "${AXIOM_EXTRA_ARGS}" 2>>"${LOGFILE}" >/dev/null
				else
					[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m katana -jc -kf all -d 2 -fs rdn -o .tmp/katana.txt "${AXIOM_EXTRA_ARGS}" 2>>"${LOGFILE}" >/dev/null
				fi
			fi
			sed -i '/^.\{2048\}./d' .tmp/katana.txt
			[[ -s ".tmp/katana.txt" ]] && cat .tmp/katana.txt | unfurl -u domains 2>>"${LOGFILE}" | grep ".$domain$" | anew -q .tmp/scrap_subs.txt
			[[ -s ".tmp/scrap_subs.txt" ]] && puredns resolve .tmp/scrap_subs.txt -w .tmp/scrap_subs_resolved.txt -r $resolvers --resolvers-trusted $resolvers_trusted -l $PUREDNS_PUBLIC_LIMIT --rate-limit-trusted $PUREDNS_TRUSTED_LIMIT --wildcard-tests $PUREDNS_WILDCARDTEST_LIMIT  --wildcard-batch $PUREDNS_WILDCARDBATCH_LIMIT 2>>"${LOGFILE}" >/dev/null
			if [[ "${INSCOPE}" = true ]]; then
				check_inscope .tmp/scrap_subs_resolved.txt 2>>"${LOGFILE}" >/dev/null
			fi
			NUMOFLINES=$(cat .tmp/scrap_subs_resolved.txt 2>>"${LOGFILE}" | grep "\.$domain$\|^$domain$" | anew subdomains/subdomains.txt | tee .tmp/diff_scrap.txt | sed '/^$/d' | wc -l)
			[[ -s ".tmp/diff_scrap.txt" ]] && cat .tmp/diff_scrap.txt | httpx -follow-host-redirects -random-agent -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info3.txt 2>>"${LOGFILE}" >/dev/null
			[[ -s ".tmp/web_full_info3.txt" ]] && cat .tmp/web_full_info3.txt | jq -r 'try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains 2>>"${LOGFILE}" | anew -q .tmp/scrap_subs.txt
			cat .tmp/web_full_info1.txt .tmp/web_full_info2.txt .tmp/web_full_info3.txt 2>>"${LOGFILE}" | jq -s 'try .' | jq 'try unique_by(.input)' | jq 'try .[]' 2>>"${LOGFILE}" > .tmp/web_full_info.txt
			end_subfunc "${NUMOFLINES} new subs (code scraping)" ${FUNCNAME[0]}
		else
			end_subfunc "Skipping Subdomains Web Scraping: Too Many Subdomains" ${FUNCNAME[0]}
		fi
	else
		end_subfunc "No subdomains to search (code scraping)" ${FUNCNAME[0]}
	fi
}

# Execute the function
sub_scraping