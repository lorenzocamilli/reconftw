#!/bin/bash

# Default config path
CONFIG_PATH="${RECONFTW_CFG}"

# Check if the config file exists
if [[ -f ${CONFIG_PATH} ]]; then
	source "${CONFIG_PATH}"
else
	echo "Error: reconftw.cfg not found at ${CONFIG_PATH}!"
	exit 1
fi

# Function to display the help menu
help_menu() {
	echo "Usage: sub_scraping.sh [OPTIONS]"
	echo ""
	echo "Options:"
	echo "    -f, --file            File containing domains for subdomain enumeration (Required)"
	echo "    -o, --output            Output file location (Optional)"
	echo "    --deep                Run in deep mode"
	echo "    --no-axiom            Disable Axiom"
	echo "    -h, --help            Display this help menu"
}

# Input validation and options parsing
while [[ $# -gt 0 ]]; do
	case $1 in
	-f | --file)
		DOMAIN_FILE="$2"
		shift
		;;
	-o | --output)
		output_file="$2"
		shift
		;;
	-h | --help)
		help_menu
		exit 0
		;;
	--deep)
		DEEP=true
		shift
		;;
	--no-axiom)
		AXIOM=false
		shift
		;;
	*)
		echo "Unknown parameter: $1"
		help_menu
		exit 1
		;;
	esac
done

# The main sub_scraping function
sub_scraping() {
	mkdir -p .tmp 2>/dev/null
	touch .tmp/scrap_subs.txt
	if [[ ! ${AXIOM} == true ]]; then
		rftw_util_resolver_quick_local
		cat ${DOMAIN_FILE} | httpx -follow-host-redirects -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info1.txt
		[[ -s ".tmp/web_full_info1.txt" ]] && cat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains | anew -q .tmp/scrap_subs.txt
		[[ -s ".tmp/probed_tmp_scrap.txt" ]] && cat .tmp/probed_tmp_scrap.txt | httpx -tls-grab -tls-probe -csp-probe -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info2.txt
		[[ -s ".tmp/web_full_info2.txt" ]] && cat .tmp/web_full_info2.txt | jq -r 'try ."tls-grab"."dns_names"[],try .csp.domains[],try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | sort -u | httpx -silent | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains | anew -q .tmp/scrap_subs.txt
		if [[ $DEEP == true ]]; then
			[[ -s ".tmp/probed_tmp_scrap.txt" ]] && katana -silent -list .tmp/probed_tmp_scrap.txt -jc -kf all -c $KATANA_THREADS -d 3 -fs rdn -o .tmp/katana.txt
		else
			[[ -s ".tmp/probed_tmp_scrap.txt" ]] && katana -silent -list .tmp/probed_tmp_scrap.txt -jc -kf all -c $KATANA_THREADS -d 2 -fs rdn -o .tmp/katana.txt
		fi
	else
		rftw_util_resolver_quick_axiom
		axiom-scan ${DOMAIN_FILE} -m httpx -follow-host-redirects -random-agent -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info1.txt "${AXIOM_EXTRA_ARGS}"
		[[ -s ".tmp/web_full_info1.txt" ]] && cat .tmp/web_full_info1.txt | jq -r 'try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains | anew -q .tmp/scrap_subs.txt
		[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m httpx -tls-grab -tls-probe -csp-probe -random-agent -status-code -threads $HTTPX_THREADS -rl $HTTPX_RATELIMIT -timeout $HTTPX_TIMEOUT -silent -retries 2 -title -web-server -tech-detect -location -no-color -json -o .tmp/web_full_info2.txt "${AXIOM_EXTRA_ARGS}"
		[[ -s ".tmp/web_full_info2.txt" ]] && cat .tmp/web_full_info2.txt | jq -r 'try ."tls-grab"."dns_names"[],try .csp.domains[],try .url' 2>/dev/null | grep "${DOMAIN}" | sed "s/*.//" | sort -u | httpx -silent | anew .tmp/probed_tmp_scrap.txt | unfurl -u domains | anew -q .tmp/scrap_subs.txt
		if [[ $DEEP == true ]]; then
			[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m katana -jc -kf all -d 3 -fs rdn -o .tmp/katana.txt "${AXIOM_EXTRA_ARGS}"
		else
			[[ -s ".tmp/probed_tmp_scrap.txt" ]] && axiom-scan .tmp/probed_tmp_scrap.txt -m katana -jc -kf all -d 2 -fs rdn -o .tmp/katana.txt "${AXIOM_EXTRA_ARGS}"
		fi
	fi
	sed -i '/^.\{2048\}./d' .tmp/katana.txt
	[[ -s ".tmp/katana.txt" ]] && cat .tmp/katana.txt | unfurl -u domains | grep ".$DOMAIN$" | anew -q .tmp/scrap_subs.txt
	[[ -s ".tmp/scrap_subs.txt" ]] && puredns resolve .tmp/scrap_subs.txt -w ${output_file} -r $resolvers --resolvers-trusted $resolvers_trusted -l $PUREDNS_PUBLIC_LIMIT --rate-limit-trusted $PUREDNS_TRUSTED_LIMIT --wildcard-tests $PUREDNS_WILDCARDTEST_LIMIT --wildcard-batch $PUREDNS_WILDCARDBATCH_LIMIT
}

# Execute the function
sub_scraping